""" Thresholding functions
Threshold functions (also called activation functions) are mathematical 
functions that determine the output of a neuron in an artificial neural 
network. They introduce non-linearity into the network, which is crucial
for learning complex patterns.
"""

# Key Types of Threshold Functions:
# Step Function -
# Simplest activation function
# Binary output (0 or 1)
# Used in basic perceptrons

# ReLU (Ramp Function) -
# Most popular in modern neural networks
# Output is x if positive, 0 if negative
# Helps solve vanishing gradient problem

# Sigmoid Function -
# Smooth, S-shaped curve
# Output between 0 and 1
# Good for probability predictions

# Why are they important?
# Introduce non-linearity into neural networks
# Enable networks to learn complex patterns
# Help in decision making and classification tasks
# Convert input signals into meaningful outputs
# Essential for neural network training

# check Activation function.py for examples
